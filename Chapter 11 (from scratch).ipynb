{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Chapter 11 (from scratch).ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyPZGNPrCs9cCADYdYJ96U+E"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"code","metadata":{"id":"P_rLRO5tOnkP","executionInfo":{"status":"ok","timestamp":1602597302872,"user_tz":-120,"elapsed":1528,"user":{"displayName":"Bob Bell","photoUrl":"","userId":"12813757050463534657"}}},"source":["#!pip install fastai --upgrade"],"execution_count":3,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"lTYFMXIdO6fN"},"source":["We have seen what `Tokenizer` and `Numericalize` do to a collection of texts, and how they're used inside the data block API, which handles those transforms for us directly using the `TextBlock`. But what if we want to apply only one of those transforms, either to see intermediate results or because we have already tokenized texts? More generally, what can we do when the data block API is not flexible enough to accomodate our particular use case? For this, we need to use fastai's *mid-level API* for processing data. The data block API is built on top of that layer, so it will allow you to do everything the data block API does and much more.\n","\n","## Going Deeper into fastai's Layered API\n","\n","The fastai library is built on a *layered API*. In the very top layer are *applications* that allow us to train a model in five lines of code, as we saw in Chapter 1. In the case of creating `DataLoaders` for a text classifier, for instance, we used this line:"]},{"cell_type":"code","metadata":{"id":"eCwhn_WiOqqS","executionInfo":{"status":"ok","timestamp":1602598203505,"user_tz":-120,"elapsed":322243,"user":{"displayName":"Bob Bell","photoUrl":"","userId":"12813757050463534657"}},"outputId":"c909e4c0-27ef-4797-ad48-381a874fb6e7","colab":{"base_uri":"https://localhost:8080/","height":17}},"source":["from fastai.text.all import *\n","\n","dls = TextDataLoaders.from_folder(untar_data(URLs.IMDB), valid='test')"],"execution_count":4,"outputs":[{"output_type":"display_data","data":{"text/html":[""],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}},{"output_type":"display_data","data":{"text/html":[""],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}}]},{"cell_type":"markdown","metadata":{"id":"0Fxu8kBPRzeZ"},"source":["The factory method `TextDataLoaders.from_folder` is very convenient when your data is arranged the exact same way as the IMBd dataset, but in practice, that often won't be the case. The data block API offers more flexibility. As we saw in the preceding chapter, we can get the same result with the following:"]},{"cell_type":"code","metadata":{"id":"ibK27G8gSTIo","executionInfo":{"status":"ok","timestamp":1602598210140,"user_tz":-120,"elapsed":6615,"user":{"displayName":"Bob Bell","photoUrl":"","userId":"12813757050463534657"}}},"source":["path = untar_data(URLs.IMDB)\n","dls = DataBlock(\n","    blocks = (TextBlock.from_folder(path), CategoryBlock),\n","    get_y = parent_label,\n","    get_items = partial(get_text_files, folders=['train', 'test']),\n","    splitter = GrandparentSplitter(valid_name='test')\n",").dataloaders(path)"],"execution_count":5,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"xN5mtsGlS53j"},"source":["But it's sometimes not flexible enough. For debugging purposes, for instance, we might need to apply just parts of the transforms that come with this data block. Or we might want to create a `DataLoaders` for an application that isn't directly supported by fastai. In this section, we will dig into the pieces that are used inside fastai to implement the data block API. Understanding these will enable you to leverage the power and flexibility of this mid-tier API.\n","\n","The mid-level API does not contain only functionality for creating `DataLoaders`. It also has the *callback* system, which allows us to customize the training loop any way we like, and the *general optimizer*. Both will be covered in Chapter 16.\n","\n","### Transforms\n","\n","When we studied tokenization and numericalization in the preceding chapter, we started by grabbing a bunch of texts:"]},{"cell_type":"code","metadata":{"id":"Bg0eqw9qTvzH","executionInfo":{"status":"ok","timestamp":1602598500079,"user_tz":-120,"elapsed":1308,"user":{"displayName":"Bob Bell","photoUrl":"","userId":"12813757050463534657"}}},"source":["files = get_text_files(path, folders=['train', 'test'])\n","txts = L(o.open().read() for o in files[:2000])"],"execution_count":6,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"n6TmsF0oUHEx"},"source":["We then showed how to tokenize them with a `Tokenizer`"]},{"cell_type":"code","metadata":{"id":"cURL_PYiUJ2y","executionInfo":{"status":"ok","timestamp":1602598605186,"user_tz":-120,"elapsed":7173,"user":{"displayName":"Bob Bell","photoUrl":"","userId":"12813757050463534657"}},"outputId":"eaa2a830-42e5-451e-d7b5-05034e50db76","colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["tok = Tokenizer.from_folder(path)\n","tok.setup(txts)\n","toks = txts.map(tok)\n","toks[0]"],"execution_count":8,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(#357) ['xxbos','xxmaj','for','starters',',','i','did',\"n't\",'even','know'...]"]},"metadata":{"tags":[]},"execution_count":8}]},{"cell_type":"markdown","metadata":{"id":"03e6bhbwUfJe"},"source":["and how to numericalize, including automatically creating the vocab for our corpus:"]},{"cell_type":"code","metadata":{"id":"rp99nuTjUrSa","executionInfo":{"status":"ok","timestamp":1602598677315,"user_tz":-120,"elapsed":1280,"user":{"displayName":"Bob Bell","photoUrl":"","userId":"12813757050463534657"}},"outputId":"73b4b1f5-6495-48e8-b5c7-36d652b31b33","colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["num = Numericalize()\n","num.setup(toks)\n","nums = toks.map(num)\n","nums[0][:10]"],"execution_count":9,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([  2,   8,  29,   0,  10,  19, 108,  42,  87, 142])"]},"metadata":{"tags":[]},"execution_count":9}]},{"cell_type":"markdown","metadata":{"id":"P4K0rZGNUyWT"},"source":["The classes also have a `decode` method. For instance, `Numericalize.decode` gives us back the string tokens:"]},{"cell_type":"code","metadata":{"id":"FJx3M4ZtU-ep","executionInfo":{"status":"ok","timestamp":1602598757752,"user_tz":-120,"elapsed":704,"user":{"displayName":"Bob Bell","photoUrl":"","userId":"12813757050463534657"}},"outputId":"1915ec35-9a4d-4fab-e22c-47399490273b","colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["nums_dec = num.decode(nums[0][:10])\n","nums_dec"],"execution_count":10,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(#10) ['xxbos','xxmaj','for','xxunk',',','i','did',\"n't\",'even','know']"]},"metadata":{"tags":[]},"execution_count":10}]},{"cell_type":"markdown","metadata":{"id":"UDCIdrLeVGED"},"source":["`Tokenizer.decode` turns this back into a single string (it may not, however, be exactly the same as the original string; this depends on whether the tokenizer is *reversible*, which th defulat word tokenizer is not at the time of the writing of the book):"]},{"cell_type":"code","metadata":{"id":"lC9M5fW2VfBL","executionInfo":{"status":"ok","timestamp":1602598871286,"user_tz":-120,"elapsed":1074,"user":{"displayName":"Bob Bell","photoUrl":"","userId":"12813757050463534657"}},"outputId":"7febd084-6083-4be8-c727-c546c486e7d2","colab":{"base_uri":"https://localhost:8080/","height":35}},"source":["tok.decode(nums_dec)"],"execution_count":11,"outputs":[{"output_type":"execute_result","data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["\"xxbos xxmaj for xxunk , i did n't even know\""]},"metadata":{"tags":[]},"execution_count":11}]},{"cell_type":"markdown","metadata":{"id":"9X_JmQAWVhhy"},"source":["`decode` is used by fastai's `show_batch` and `show_results`, as well as some other inference methods, to convert predictions and mini-batches into a human-understandable representation.\n","\n","For each of `tok` or `num` in the preceding examples, we created an object called the `setup` method (which trains the tokenizer if needed for `tok` and creates the vocab for `num`), applied it to our raw texts (by calling the object as a function), and then finally decoded the result back to an understandable representation. These steps are needed for most data preprocessing tasks, so fastai provides a class that encapsulates them. This is the `Transform` class. Both `Tokenize` and `Numericalize` are `Transforms`.\n","\n","In general, a `Transform` is an object that behaves like a function and has an optional `setup` method that will initialize an inner state (like the vocab inside `num`) and an optional `decode` method that will reverse the function (this reversal may not be perfect, as we saw with `tok`).\n","\n","A good example of `decode` is found in the `Normalize` transform that we saw in Chapter 7: to be able to plot the images, its `decode` method undoes the normalization (i.e., it multiplies by the standard deviation and adds back the mean). On the other hand, data augmentation transforms do not have a `decode` method, since we want to show the effects on images to make sure the data augmentation is working as we want.\n","\n","A special behavior of `Transforms` is that they always get applied over tuples. In general, our data is always a tuple `(input, target)` (sometimes with more than one input or more than one target). When applying a transform on an item like this, such as `Resize`, we don't want to resize the tuple as a whole; instead, we want to resize the input (if applicable) and the target (if applicable) separately. It's the same for batch transforms that do data augmentation: when the input is an image and the target is a segmentation mask, the transform needs to be applied (the same way) to the input and the target.\n","\n","We can see this behavior if we pass a tuple of texts to `tok`:"]},{"cell_type":"code","metadata":{"id":"kIWtnJ51X74-","executionInfo":{"status":"ok","timestamp":1602599517635,"user_tz":-120,"elapsed":676,"user":{"displayName":"Bob Bell","photoUrl":"","userId":"12813757050463534657"}},"outputId":"d6a5b17a-b58a-402a-9d93-82b9f22fde8d","colab":{"base_uri":"https://localhost:8080/","height":51}},"source":["tok((txts[0], txts[1]))"],"execution_count":12,"outputs":[{"output_type":"execute_result","data":{"text/plain":["((#357) ['xxbos','xxmaj','for','starters',',','i','did',\"n't\",'even','know'...],\n"," (#131) ['xxbos','xxmaj','despite','xxmaj','disney',\"'s\",'best','efforts',',','this'...])"]},"metadata":{"tags":[]},"execution_count":12}]},{"cell_type":"markdown","metadata":{"id":"9UUdPysEX_qc"},"source":["### Writing Your Own Transform\n","\n","If you want to write a custom transform to apply to your data, the easiest way is to write a function. As you can see in this example, a `Transform` will be applied only to a matching type, if a type is provided (otherwise, it will always be applied). In the following code, the `:int` in the function signature means that `f` gets applied only to `ints`. That's why `tfm(2.0)` returns 2.0, but `tfm(2)` returns 3 here:"]},{"cell_type":"code","metadata":{"id":"pO3u6_eLYpg7","executionInfo":{"status":"ok","timestamp":1602599711594,"user_tz":-120,"elapsed":1047,"user":{"displayName":"Bob Bell","photoUrl":"","userId":"12813757050463534657"}}},"source":["def f(x:int):\n","  return x + 1"],"execution_count":13,"outputs":[]},{"cell_type":"code","metadata":{"id":"eaXRvp1BYu7R","executionInfo":{"status":"ok","timestamp":1602599738906,"user_tz":-120,"elapsed":1030,"user":{"displayName":"Bob Bell","photoUrl":"","userId":"12813757050463534657"}}},"source":["tfm = Transform(f)"],"execution_count":16,"outputs":[]},{"cell_type":"code","metadata":{"id":"f99wx5TnYwk_","executionInfo":{"status":"ok","timestamp":1602599749026,"user_tz":-120,"elapsed":488,"user":{"displayName":"Bob Bell","photoUrl":"","userId":"12813757050463534657"}},"outputId":"9d0f7c3b-48ba-4343-9c74-56ead660bfae","colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["tfm(2), tfm(2.0)"],"execution_count":17,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(3, 2.0)"]},"metadata":{"tags":[]},"execution_count":17}]},{"cell_type":"markdown","metadata":{"id":"Wqgzc2R1Y4Lz"},"source":["Here, `f` is converted to a `Transform` with no `setup` and no `decode` method.\n","\n","Python has a special syntax for passing a function (like `f`) to another function (or something that behaves like a function, known as a *callable* in Python), called a *decorator*. A decorator is used by prepending a callable with @ and placing it before a function definition. The following is identical to the previous code:"]},{"cell_type":"code","metadata":{"id":"H7nYjXiWZdm3","executionInfo":{"status":"ok","timestamp":1602599943970,"user_tz":-120,"elapsed":1048,"user":{"displayName":"Bob Bell","photoUrl":"","userId":"12813757050463534657"}},"outputId":"d564086e-0337-4a83-96af-5b0d1ee45378","colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["@Transform\n","def f(x:int):\n","  return x+1\n","f(2), f(2.0)"],"execution_count":18,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(3, 2.0)"]},"metadata":{"tags":[]},"execution_count":18}]},{"cell_type":"markdown","metadata":{"id":"UdgtmOhJZnqd"},"source":["If you need either `setup` or `decode`, you will need to subclass `Transform` to implement the actual encoding behavior in `encodes`, then (optionally) the setup behavior in `setups` and the decoding behavior in `decodes`:"]},{"cell_type":"code","metadata":{"id":"Ae9SErsAaqFq","executionInfo":{"status":"ok","timestamp":1602600487452,"user_tz":-120,"elapsed":911,"user":{"displayName":"Bob Bell","photoUrl":"","userId":"12813757050463534657"}}},"source":["class NormalizeMean(Transform):\n","  def setups(self, items):\n","    self.mean = sum(items)/len(items)\n","  \n","  def encodes(self, x):\n","    return x-self.mean\n","\n","  def encodes(self, x):\n","    return x+self.mean"],"execution_count":21,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"n1if51tIa-ZU"},"source":["Here, `NormalizeMean` will initialize a certain state during the setup (the mean of all elements passed); then the transformation is to subtract that mean. For decoding purposes, we implement the reverse of that transformation by adding the mean. Here is an example of `NormalizeMean` in action:"]},{"cell_type":"code","metadata":{"id":"XMWkB32ObV_c","executionInfo":{"status":"ok","timestamp":1602600518168,"user_tz":-120,"elapsed":1206,"user":{"displayName":"Bob Bell","photoUrl":"","userId":"12813757050463534657"}},"outputId":"19fa5ca4-9715-40fd-f688-802d7775ac81","colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["tfm = NormalizeMean()\n","tfm.setup([1,2,3,4,5])\n","start = 2\n","y = tfm(start)\n","z = tfm.decode(y)\n","tfm.mean, y, z"],"execution_count":23,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(3.0, 5.0, 5.0)"]},"metadata":{"tags":[]},"execution_count":23}]},{"cell_type":"markdown","metadata":{"id":"8B5ih7cRblMh"},"source":["# Look into why this differs from the book. [13 October 2020 @ 1649 CET]"]}]}