{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"lesson2.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyMbMYkhOl/rg3D9HvyUDaYU"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"JxnG_rOi2SUO","colab_type":"text"},"source":["# Lesson 2 General Notes (incomplete)\n","* After ML training, you end up with a normal program that transforms inputs into outputs.\n","* Classification model predicts class/category. Regression predicts a numeric quantity. Regression does not necessarily refer to *linear regression*.\n","*  `valid_pct=0.2` defines 20% as the validation set, which helps us to diagnose **overfitting**.\n","* learner object helps us to find the parameters that best match the architecture to the labels in the data.\n","  * `learn = cnn_learner(dls, resnet34, metrics=error_rate)`\n","  * resnet34: name of architecture good for image classification.\n","  * `metrics=error_rate`: what to print after each epoch (looking at every image in the dataset once). Metric measures quality of model's prediction using validation set. Loss is not necessarily the same as the metric. You need a loss function where if I change the parameter slightly, we can see if the loss gets better/worse; unfortunately, accuracy/error_rate doesn't give us this. Metric is what we care about while the loss is what the model uses to assess its performance.\n","* Overfitting is an important and challenging issue. It results from the model cheating by memorizing results and not generalizing from our dataset.\n","  * [Question] Is overfitting detected as training loss < validation loss? This could be true, but there could be cases where the validation loss gets worse but the validation accuracy improves. In short, we should focus on validation metric getting worse, not the loss function getting worse.\n","* However, just having a validation set doesn't mean that we can't cheat. We can overfit to the validation set by looking at its results and selectively changing the model to improve validation metric. We should set aside the test set that is not used for training or metrics. It is only used once all training is completed.\n","* Pulling out validation and test sets can be subtle. For example, we cannot randomly remove points from a time series dataset. In this case, you want to chop off the end of the time series for the validation/test set.\n","* `learn.fine_tune()` does transfer learning - using a pretrained model for a task different to what it was originally trained for. Important for less compute and data and getting better accuracy.\n","* [question] what is difference between loss, error, and metric?\n","  * Model to predict how old a cat or dog is. Metric could be how many years were you off by?\n","  * Model to predict cat vs. dog. Metric could be what percentage of the time am I wrong? This metric is called the error rate. Error is one particular metric.\n","  * Loss is what ML model uses to assess its performance  to improve the model by updating the weights/parameters. It tracks closely the metric you care about.\n","* Fine tuning: transfer learning technique where parameters of a pretrained model are updated by training for additional epochs using a different task to that used for pretraining.\n","  1. Use one *epoch* to fit just those parts of the model necessary to get the new random *head* to work correctly with your dataset\n","  2. Use the number of epochs requested when calling the method to fit the entire model, updating the weights of the later layers (especially the head) faster than the earlier layers (which generally don't require as many changes from the pretrained weights)\n","* Why does transfer learning work so well? Zeiler and Fergus visualized CNN layers.\n","  * First layer could find diagonal lines, gradients, green patches of color, etc. These filters are very general.\n","  * Second layer combines features from the first layer to find circles, edges, corners, repeating curving patterns, semicircles, etc.\n","  * Third layer combines features from the second layer. It can find text, repeating geometric patterns, etc.\n","  * Fourth layer can find dog faces.\n","  * In each layer, we get multiplicately complex features. When using transfer learning, we can use pre-learned features for our specific task.\n","* CV techniques are not just for recognizing photos.\n","  * Sounds can be turned into pictures by representing their features over time.\n","  * Anti-fraud detection.\n","  * Looking at different viruses and turning them into pictures.\n","* Useful terms and what they mean (from the book)\n","  * label: data we are trying to predict, such as \"dog\" or \"cat\"\n","  * architecture: The *template* of the mode we're trying to fit; the mathematical function we're passing the input data and parameters to\n","  * model: architecture + parameters\n","  * parameters: values in the model that change what task it can do and are updted through model training\n","  * fit: Update parameters of the model s.t. predictions of the model using the input data match the target labels\n","  * train: Synonym for *fit*\n","  * pretrained model: Model already trained (typically using a large dataset) and will be fine-tuned\n","  * fine tune: Update a pretrained model for a different task\n","  * epoch: One complete pass through the input data\n","  * loss: Measure of how good the model is, chosen to drive training via stochastic gradient descent\n","  * metric: Measurement of how good the model is, using validation set, chosen for human consumption\n","  * validation set: Dataset held out from training set to measure how good the model is\n","  * training set: Dataset used for fitting the model; it does not include data from the validation (or test) set(s)\n","  * overfitting: Training a model that *remembers* specific features of the input data rather than generalizing to unseen data\n","  * CNN: Convolutional neural network; type of neural work that works well for computer vision\n","* It is important to look at the questionnaire.\n","* Further research section are fun and interesting in the beginning. It requires additional work/research beyond reading the chapter.\n","* For future chapters (2+), rewrite notebooks from scratch on a new dataset. Through lots of practice and failure does one gain the intuition to train a model.\n","* [Question]: Are filters independent? Will pre-trained models fine-tuned get less good at predicting original images that they were trained on?\n","  * If you start with ImageNet model and finetune on dogs vs. cats and get something good, it may not be good as ImageNet model after that.\n","  * This is catastrophic forgetting.\n","  * If you want something good at the new task and previous task, you need to put in examples from the previous task in your training as well.\n","* [Question] What are differences between parameters and hyperparameters?\n","  * Parameters are the numbers which change what the model/architecture does.\n","  * Hyperparameters are the choices about the fitting process.\n","* Whole book will cover 2-3 courses (14-21 lessons).\n","  * Second part of the course covers putting things into production. What are capabilities and limitations of deep learning. What makes sense to put into production.\n","  * In first 2-3 lessons/chapters are designed for everyone.\n","  * Book/course covers vision, text, tabular, and recsys.\n","  * DL good for tabular when there is high cardinality.\n","  * DL in text is good for classification but bad for conversation.\n","  * For recommendation systems and collaborative filtering, DL used for predictions but not recommendations.\n","  * DL good for multi-modal data. For example, image captioning is pretty good.\n","  * Other: can be creative. For example, ULMFit is also fantastic for protein analysis.\n","* [Question] What other pre-trained models can we use? Only ImageNet or others?\n","  * Search: model zoo deep learning\n","  * Search: pretrained models deep learning\n","  * Most pretrained models are still focused on ImageNet and similar datasets.\n","* This course will involve learning how to read papers.\n","* How to decide if there is a relationship?\n","  * Pick a \"null hypothesis\": e.g., no relationship\n","  * Gather data of independent and dependent variables\n","  * What % of the time do we see relationship by chance?\n","  * A p-value is the probability of an observed (or more extreme) result assuming that the null hypothesis is true.\n","  * There are problems with p-values. However, multivariate models allow us to be more confident. \n","* \n"]},{"cell_type":"code","metadata":{"id":"8KDWbnC3budE","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1598121136159,"user_tz":-120,"elapsed":680,"user":{"displayName":"Bob Bell","photoUrl":"","userId":"12813757050463534657"}}},"source":["#!pip install fastai --upgrade\n","#!pip install nbdev\n","#!pip install azure-cognitiveservices-search-imagesearch"],"execution_count":7,"outputs":[]},{"cell_type":"code","metadata":{"id":"8SZ7VIzUbk0W","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1598121139280,"user_tz":-120,"elapsed":753,"user":{"displayName":"Bob Bell","photoUrl":"","userId":"12813757050463534657"}}},"source":["from utils import *\n","from fastai.vision.widgets import *"],"execution_count":8,"outputs":[]},{"cell_type":"code","metadata":{"id":"h9ikgBY2bra4","colab_type":"code","colab":{}},"source":[""],"execution_count":null,"outputs":[]}]}